# 신경망 학습

__학습__이란 훈련데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻함

- 학습의 목표 : 신경망이 학습할 수 있도록 해주는 지표가 손실함수
- 손실 함수의 결괏괎을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표 

## 데이터에서 학습한다!

신경망의 특징은 데이터를 보고 학습할 수 있다. 데이터에서 학습한다는 것은 가중치 매개변수의 값을 데이터를 보고 자동으로 결장한다는 뜻이다.

> 선형 분리 가능 문제라면 데이터로부터 자동으로 학습할 수 있다(퍼셉트론 수렴정리). 하지만 비선형 분리 문제는 (자동으로) 학습 할 수 없다.

## 데이터 주도 학습

MNIST를 예로 들어, 데이터를 활용한다면. 이미지에서 __특징(feature)__을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다.

> 입력 데이터(입력 이미지)에서 본질적인 데이터를 정확하게 추출할 수 있도록 설계된 변환기. 컴퓨터 비전에서는 SIFT, SURF, HOG 등. SVM, KNN등으로 학습할 수 있음

기계학습의 두 가지 접근법

- 사람이 생각한 특징(SIFT, HOG 등) -> 기계학습(SVM, KNN등)
- 신경망 딥러닝

신경망은 이미지를 '있는 그대로' 학습한다. 

> 딥러닝을 종단간 기계학습이라고도 한다. 종단간은 '처음부터 끝까지'라는 의미로 데이터에서 목표한 결과를 얻는 다는 뜻

### 훈련데이터와 시험 데이터

훈련 데이터와 시험 데이터를 나눠야 한다. 우선 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾는다. 시험데이터를 사용하여 훈련한 모델의 실력을 평가하는 것이다. 훈련 데이터와 시험데이터를 나누는 이유는 범용적으로 사용하기 위해서다.

> 한 데이터셋에만 지나치게 최적화된 상태를 __오버피팅__이라고 한다.

## 손실 함수

신경망은 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색한다. 신경망 학습에서 사용하는 지표는 __손실 함수__라고 함. 일반적으로 __평균 제곱 오차__와 __교차 엔트로피 오차__를 사용한다.

> 손실 함수는 신경망 성능의 '나쁨'을 나타내는 지표로 신경망이 훈련데이터를 얼마나 잘 처리하지 못하느냐를 나타낸다.

### 평균 제곱 오차

가장 많이 쓰이는 손실 함수는 평균 제곱 오차.  ([위키피디아](https://en.wikipedia.org/wiki/Mean_squared_error)) [^1]

y_k는 신경망의 출력(신경망이 추정한 값), t_k는 정답 레이블, k는 데이터의 차원 수를 나타냄.

![Imgur](http://i.imgur.com/yrAfh5z.png)

```python
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```

정답 레이블인  t는 정답을 1로 표시함. 이처럼 한 원소만 1로 하고 그 외에는 0으로 나타내는 표기법을 __원-핫 인코딩__이라고 했음.



```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

첫 번째의 예는 정답이 2, 신경망의 출력도 '2'에서 가장 높은 경우. 두 번째 예에서도 정답은 똑같이 '2'지만, 신경망의 출력은 7에서 가장 높음. 다라서 첫번째의 예의 손실함수 쪽이 __출력이 작으면서 오차도 작은것__ 을 알고 있음. 즉 정답에 더 가깝다고 볼 수 있다.

```python
>>> t = [0,0,1,0,0,0,0,0,0,0]
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> mean_squared_error(np.array(y), np.array(t))
0.097500000000000031
>>> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
>>> mean_squared_error(np.array(y), np.array(t))
0.59750000000000003
```

### 교차 엔트로피 오차

![Imgur](http://i.imgur.com/JcunQcj.png)

교차 엔트로피 오차에서도 원-핫 인코딩이다. 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다. 자연로그(y = log x ) 라고 하는데 `ln x` 아닌가? [^2]

```python
>>> def cross_entropy_error(y, t):
...     delta = 1e-7 # np.log 함수에 0을 입력하면 -inf가 되어 계산 진행 불가, 아주 작은 값을 더해 0이 되지 않도록 함
...     return -np.sum(t * np.log(y + delta))
...
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> cross_entropy_error(np.array(y), np.array(t))
0.51082545709933802
>>> cross_entropy_error(np.array(y), np.array(t))
2.3025840929945458
```

첫 번째 예는 정답일 때의 출력이 0.6인 경우로, 교차 엔트로피 오차는 0.51로 나왔음. 두 번째 결과(오차 값)보다 더 작은 첫번째 추정이 정답일 가능성이 높다고 판단했으므로 평균 제곱 오차의 판단과 일치함. 

### 미니배치 학습

기계 학습 문제는 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아내는 것이다. 이렇게 하려면 훈련 데이터를 대상으로 손실 함수 값을 구해야 함.

[^1]: 1/2 가 아니고 1/n 라고 오탈자 제보에 등록되어 있다.
[^2]: p.115, 2번째 줄 no.log -> np.log 