{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6장 학습 관련 기술\n",
    "\n",
    "- 매개변수의 최적화 방법\n",
    "- 가중치 매개변수 초깃값\n",
    "- 하이퍼파라미터 설정방법\n",
    "\n",
    "### 오버피팅의 대응책\n",
    "\n",
    "- 가중치 감소\n",
    "- 드롭아웃\n",
    "\n",
    "### 매개변수의 갱신 방법\n",
    "- 확률적 경사 하강법(SGD) 다시 보기\n",
    "- 모멘텀(Momentum)\n",
    "- AdaGrad\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률적 경사하강법(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        \"\"\"lr은 학습률\"\"\"\n",
    "        self.lr = lr\n",
    "        \n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W \\rightarrow W - \\eta\\frac{\\delta L}{\\delta W}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W는 갱신할 가중치 매개변수, 우변의 편미분은 W에 대한 손실함수의 기울기, 에타는 학습률이다. SGD에는 단점이 있는데 학습이 비효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient Descent Optimization Algorithms at Long Valley](http://i.imgur.com/2dKCQHh.gif?1)\n",
    "\n",
    "![Gradient Descent Optimization Algorithms at Saddle Point](http://i.imgur.com/NKsFHJb.gif?1)\n",
    "\n",
    "![Gradient Descent Optimization Algorithms at Beale's Function](http://i.imgur.com/pD0hWu5.gif?1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모멘텀\n",
    "\n",
    "$$ \n",
    "v \\leftarrow \\alpha v - \\eta \\frac{\\delta L}{\\delta W} \\\\\n",
    "W \\leftarrow W + v \n",
    "$$\n",
    "\n",
    "V는 물리에서 말하는 속도에 해당한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "참고문헌에서 본 코드로 보면 간단하다\n",
    "SGD(Vanilla update)\n",
    "```python\n",
    "x += - learning_rate * dx \n",
    "```\n",
    "\n",
    "Momentum update\n",
    "```python\n",
    "v = mu * v - learning_rate * dx # integrate velocity\n",
    "x += v # integrate position\n",
    "```\n",
    "\n",
    "각각의 매개변수 갱신방법에 대한 그래프 모양은 위의 그림을 참고하도록 하자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "신경망 학습에서는 당연하게도 학습률이 중요하다. 너무 작으면 학습시간이 길어지고, 크면 발산하여 올바른 학습을 할 수 없기 때문. 이를 해결하는 간단한 방법은 일괄적으로 낮추는 것.\n",
    "\n",
    "AdaGrad는 개별 매개변수에 adative하게 학습률을 조정하여 학습을 진행한다\n",
    "\n",
    "$$ h \\leftarrow h + \\frac{\\delta L}{\\delta W} \\odot \\frac{\\delta L}{\\delta W} $$\n",
    "$$ W \\leftarrow W + \\eta \\frac{1}{\\sqrt{h}} \\frac{\\delta L}{\\delta W}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h라는 새로운 변수가 나오는데, 기존 기울기 값을 제곱하여 더해줌 (\\odot은 행렬의 원소별 곱셈을 의미함), 매개변수를 갱신할때 \\frac{1}{\\sqrt{h}}를 곱해 학습률을 조정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉 AdaGrad는 과거의 기울기를 제곱하여 계속 더해간다. 그래서 학습을 진행할 수록 강도가 약해짐.\n",
    "\n",
    "RMSProp은 AdaGrad의 단점을 개선한 방법. 먼 과거의 기울기는 서서이 잊고 새로운 기울기 정보를 크게 반영한다. 이를 지수이동평균이라 하는데, 이것 또한 위에 그림이 있으니 참고하도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고하면 좋은 글들\n",
    "- [Gradient Descent Optimization Algorithms 정리](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)\n",
    "- [CS231n Convolutional Neural Networks for Visual Recognition](http://aikorea.org/cs231n/neural-networks-3/#sgd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
