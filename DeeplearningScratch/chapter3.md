# 신경망

__가중치를 설정 작업은 사람이 수동으로 해야한다__ 하지만 신경망은 이것을 해결해줌.

신경망의 중요한 성질은 __가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력__

## 퍼셉트론에서 신경망으로

신경망을 예로 들때 입력층, 은닉층, 출력층이 있으며, 은닉층은 숨겨져 있기때문에 은닉층임. 참고로 은닉층이 2개 이상인 신경망 구조를 딥러닝이라 __부를 수도__ 있음.

퍼셉트론에 편향을 명시하면 기존 퍼셉트론 모형에 가중치가 b 이고 입력이 1인 뉴런이 추가됨.

기존 퍼셉트론 식을 다음과 같은 간결한 형태로 다시 작성할 수 있음. <br>
![Imgur](http://i.imgur.com/t7M8d7Q.png)

## 활성화 함수의 등장
__h(x)__ 를 활성화 함수(activation function), 입력신호의 총합이 활성화를 일으키지를 정하는 역할을 함.

위의 식을 다 시 써보면 다음과 같이 된다. <br>
a = b + w1x1 + w2x2 [^1] <br>
y = h(a)

가중치가 달린 입력 신호와 편향의 총합을 계산하고, 이를 a 라 함. a를 함수 h()에 넣어 y를 출력하는 흐름. 그림 3-4 참고

이 책에선 __뉴런__과 __노드__를 같은 의미로 사용함.


> 단순 퍼셉트론은 단층 계단 함수(임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델

> 다층 퍼셉트론은 신경망(여러 층으로 구성되고 시그모이드 함수 등의 매끈함 활성화 함수를 사용하는 모델)을 가리킴

##  활성화 함수
활성화함수는 임계값을 경계로 출력이 바뀌는데, 이런 함수를 계단 함수(step function)이라 함.

퍼셉트론은 __이미 계단함수를 채용__! 활성화 함수를 계단 함수에서 __다른 함수로 변경__ 하는 것이 신경망의 세계로 나아가는 열쇠!
### 시그모이드 함수

h(x) = 1 / 1 + exp(-x)

신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하여 뉴런에 전달. 

퍼셉트론과 신경망의 주된 차이는 __활성화 함수__



## 계단 함수 구현하기

```python
def step_function(x):
   	if x > 0:
        return 1
   	else:
        return 0
# 이렇게 하면 쉽지만 넘파이 배열을 받도록 다시 구현
def step_function(x):
    y = x > 0
    return y.astype(np.int)
# https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html
# 원하는 자료형으로 변환할떄는 astype() 메서드를 사용하면 됨.
```

계단함수의 그래프와 시그모이드 함수 그래프는 ipython 으로 작성하였음

## 시그모이드 함수와 계단 함수 비교

- 시그모이드 함수는 부드러운 곡선형이라 __입력에 따라 출력이 연속적으로 변화__ 
- 계단 함수는 __0을 경계로 출력이 값이 변화__ 
- 공통점은 입력이 작을때는 출력이 0에 가깝고(혹은 0), 입력이 커지면 출력이 1에 가까워지는(혹은 1) 구조

또한 시그모이드 함수와 계단함수의 공통점은 __비선형 함수__.

신경망에서는 활성화 함수로 __비선형 함수를 사용해야함__. 선형함수를 이용하면 신경망의 (은닉)층을 깊게 하는 의미가 없어지기 때문이다. 달리 말하면 은닉층이 없는 네트워크로도 똑같은 기능을 할 수 있다는 점이다. 

### 다차원 배열의 계산



```python
# 1차원
>>> import numpy as np
>>> A = np.array([1, 2, 3, 4])
>>> print(A)
[1 2 3 4]
>>> np.ndim(A)
1
>>> A.shape
(4,)
>>> A.shape[0]
4
# 2차원
>>> B = np.array([[1,2], [3,4], [5,6]])
>>> print(B)
[[1 2]
[3 4]
[5 6]]
>>> np.ndim(B)
2
>>> B.shape
(3,2)
# 2×2 행렬 계산
>>> A = np.array([[1,2], [3,4]])
>>> A.shape
(2, 2)
>>> B = np.array([[5,6], [7,8]])
>>> B.shape
(2, 2)
>>> np.dot(A, B) # 내적 계산은 np.dot()으로 계산함
array([[19, 22]
        43, 50])
```

행렬의 곱에서는 대응하는 차원의 원소 수를 일치시켜야 함

3 × 2 →→ 2 × 4	= 3 × 4

↳→→→→→⤴︎

## 신경망의 내적



```python
>>> X = np.array([1,2])
>>> X.shape
(2,)
>>> W = np.array([[1, 3, 5], [2, 4, 6]])
>>> print(W)
[[1 3 5]
  2 4 6]]
>>> W.shape
(2, 3)
>>> Y = np.dot(X, W)
>>> print(Y)
[5 11 17]
# np.dot은 중요함 (for로 노가다 할것이 아니므로) 아무튼 중요함!
```



[^1] : 책을 자세히 보면 + 가 아니라 =+ 다.  